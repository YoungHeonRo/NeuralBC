{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절대 임포트 설정\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "# 필요한 라이브러리들을 임포트\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "define all the constants\n",
    "\"\"\"\n",
    "numEpochs = 200\n",
    "seriesLength = 500000\n",
    "backpropagationLength = 250\n",
    "stateSize = 4\n",
    "numClasses = 10\n",
    "echoStep = 3\n",
    "batchSize = 250\n",
    "num_batches = seriesLength // batchSize // backpropagationLength\n",
    "print(num_batches)\n",
    "param_sparsity = 0.01\n",
    "param_weight_decay = 0.0001\n",
    "param_sparse_panelty = 3\n",
    "\n",
    "learning_rate_Gradient_Descent = 0.5\n",
    "softmax_classifier_iterations = 1000 # Softmax Classifier iteration 횟수 \n",
    "  \n",
    "display_step = 1        # 몇 Step마다 log를 출력할지 결정한다.\n",
    "examples_to_show = 10   # reconstruct된 이미지 중 몇개를 보여줄지를 결정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate data\n",
    "\"\"\"\n",
    "def generateData():\n",
    "    #x = np.array(np.random.choice(25, seriesLength, p=[0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]))\n",
    "    x = np.random.random((5, 50000))\n",
    "    \n",
    "    y = np.roll(x, echoStep)\n",
    "    y[0:echoStep] = 0\n",
    "    \n",
    "    print(\"x :\", x.shape)\n",
    "    print(\"y :\", y.shape)\n",
    "    print(\"batchSize : \", batchSize)\n",
    "\n",
    "    x = x.reshape((batchSize, -1))\n",
    "    y = y.reshape((batchSize, -1))\n",
    "    \n",
    "    print(\"x_reshape :\", x.shape)\n",
    "    print(\"y_reshape :\", y.shape)\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(weights):\n",
    "    return tf.nn.l2_loss(weights)\n",
    "\n",
    "def Single_Layer_Autoencoder(Input_Layer, n_input, n_hidden):\n",
    "    with tf.name_scope(\"Hidden_Layer\") as scope:\n",
    "        # 히든 레이어 1을 위한 Weights와 Biases\n",
    "        W_ih = tf.Variable(tf.random_normal([n_input, n_hidden]), name = \"Weight_IH\")  # IH = Input_Hidden\n",
    "        b_ih = tf.Variable(tf.random_normal([n_hidden]), name = \"Bias_IH\")\n",
    "        H_Layer = tf.nn.sigmoid(tf.matmul(Input_Layer, W_ih) + b_ih, name = \"Hidden\")     # 히든레이어 1의 activation (sigmoid 함수를 사용)\n",
    "               \n",
    "    with tf.name_scope(\"Reconstructed_Layer\") as scope:\n",
    "        # Output 레이어를 위한 Weights와 Biases\n",
    "        W_hr = tf.Variable(tf.random_normal([n_hidden, n_input]), name = \"Weight_HR\")  # HR = Hidden_Reconstructed\n",
    "        b_hr = tf.Variable(tf.random_normal([n_input]), name = \"Bias_HR\")\n",
    "        X_reconstructed = tf.nn.sigmoid(tf.matmul(H_Layer,W_hr) + b_hr, name = \"Hidden_HR\")   # Output 레이어의 activation (sigmoid 함수를 사용)\n",
    "    \n",
    "    with tf.name_scope(\"cost\") as scope:        \n",
    "    # Autoencoder Optimization을 위한 파라미터들\n",
    "        average_act_hidden = tf.reduce_mean(H_Layer,axis=0)   #Average hidden layer over all data points in X, Page 14 in https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf\n",
    "        diff = Input_Layer - X_reconstructed\n",
    "        KL = Kullback_Leibler_divergence(rho, average_act_hidden)\n",
    "        cost= 0.5*tf.reduce_mean(tf.reduce_sum(tf.pow(diff,2),axis=1)) + 0.5*Lambda*(tf.nn.l2_loss(W_ih) + tf.nn.l2_loss(W_hr)) + Beta*tf.reduce_sum(KL)\n",
    "        cost_summary = tf.summary.scalar(\"cost\", cost)\n",
    "        \n",
    "\n",
    "    return H_Layer, X_reconstructed, cost\n",
    "\n",
    "def Kullback_Leibler_divergence(rho, rho_hat):\n",
    "    return rho * tf.log(rho) - rho * tf.log(rho_hat) + (1 - rho) * tf.log(1 - rho) - (1 - rho) * tf.log(1 - rho_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_series : 250\n",
      "predictions_series : 250\n"
     ]
    }
   ],
   "source": [
    "rho = param_sparsity\n",
    "Lambda = param_weight_decay\n",
    "Beta = param_sparse_panelty\n",
    "\n",
    "# 학습에 필요한 변수들 설정\n",
    "with tf.name_scope(\"Tensor_initialize\"):\n",
    "       \n",
    "    batchXHolder = tf.placeholder(tf.float32, [batchSize, backpropagationLength], name=\"x_input\")\n",
    "    batchYHolder = tf.placeholder(tf.int32, [batchSize, backpropagationLength], name=\"y_input\")    \n",
    "        \n",
    "with tf.name_scope(\"Auto_Encoder_01\"):\n",
    "    H1, O1, cost1 = Single_Layer_Autoencoder(batchXHolder, batchSize, batchSize)\n",
    "    optmzr1 = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(cost1)\n",
    "\n",
    "with tf.name_scope(\"Auto_Encoder_02\"):\n",
    "    H2, O2, cost2 = Single_Layer_Autoencoder(H1, batchSize, batchSize)\n",
    "    optmzr2 = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(cost2)\n",
    "\n",
    "with tf.name_scope(\"Auto_Encoder_03\"):\n",
    "    H3, O3, cost3 = Single_Layer_Autoencoder(H2, batchSize, batchSize)\n",
    "    optmzr3 = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(cost3)\n",
    "\n",
    "with tf.name_scope(\"Auto_Encoder_04\"):\n",
    "    H4, O4, cost4 = Single_Layer_Autoencoder(H3, batchSize, batchSize)\n",
    "    optmzr4 = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(cost4)\n",
    "\n",
    "with tf.name_scope(\"Long_Short_Memory_Networks\"):\n",
    "    cellState = tf.placeholder(tf.float32, [batchSize, stateSize])\n",
    "    hiddenState = tf.placeholder(tf.float32, [batchSize, stateSize])\n",
    "    initState = rnn.LSTMStateTuple(cellState, hiddenState)\n",
    "    \n",
    "    W = tf.Variable(np.random.rand(stateSize+1, stateSize), dtype=tf.float32, name=\"weight1\")\n",
    "    bias1 = tf.Variable(np.zeros((1,stateSize)), dtype=tf.float32)\n",
    "    W2 = tf.Variable(np.random.rand(stateSize, numClasses),dtype=tf.float32, name=\"weight2\")\n",
    "    bias2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "    \n",
    "    tf.summary.histogram(name=\"LSTM_weights\", values=W)\n",
    "        \n",
    "    # Unpack columns\n",
    "    inputsSeries = tf.split(axis=1, num_or_size_splits=backpropagationLength, value=H4)\n",
    "    labelsSeries = tf.unstack(batchYHolder, axis=1)\n",
    "        \n",
    "    # Forward passes\n",
    "    cell = rnn.BasicLSTMCell(stateSize, state_is_tuple=True)\n",
    "    statesSeries, currentState = rnn.static_rnn(cell, inputsSeries, initState)\n",
    "    \n",
    "    # calculate loss\n",
    "    logits_series = [tf.matmul(state, W2) + bias2 for state in statesSeries]\n",
    "    print(\"logits_series :\",len(logits_series))\n",
    "    predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "    print(\"predictions_series :\", len(predictions_series))\n",
    "    losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits) for logits, labels in zip(logits_series,labelsSeries)]\n",
    "    total_loss = tf.reduce_mean(losses, name=\"total_loss\")\n",
    "    train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss, name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plot computation\n",
    "\"\"\"\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batchSeriesIdx in range(5):\n",
    "        oneHotOutputSeries = np.array(predictions_series)[:, batchSeriesIdx, :]\n",
    "        singleOutputSeries = np.array([(1 if out[0] < 0.5 else 0) for out in oneHotOutputSeries])\n",
    "\n",
    "        plt.subplot(2, 3, batchSeriesIdx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, backpropagationLength, 0, 2])\n",
    "        left_offset = range(backpropagationLength)\n",
    "        plt.bar(left_offset, batchX[batchSeriesIdx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batchSeriesIdx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, singleOutputSeries * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : (5, 50000)\n",
      "y : (5, 50000)\n",
      "batchSize :  250\n",
      "x_reshape : (250, 1000)\n",
      "y_reshape : (250, 1000)\n",
      "New data, epoch 0\n",
      "Step 0 Loss 2.3004253\n",
      "Step 1 Loss 1.7458103\n",
      "Step 2 Loss 1.2235825\n",
      "Step 3 Loss 0.68826276\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Level value of 2 is too high.  Maximum allowed is 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-caadacc72f46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mbatchY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mcoeffs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwavedec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'haar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mreconstruct_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaverec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'haar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pywt\\_multilevel.py\u001b[0m in \u001b[0;36mwavedec\u001b[1;34m(data, wavelet, mode, level, axis)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Axis greater than data dimensions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwavelet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mcoeffs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pywt\\_multilevel.py\u001b[0m in \u001b[0;36m_check_level\u001b[1;34m(size, dec_len, level)\u001b[0m\n\u001b[0;32m     38\u001b[0m             raise ValueError(\n\u001b[0;32m     39\u001b[0m                 \"Level value of %d is too high.  Maximum allowed is %d.\" % (\n\u001b[1;32m---> 40\u001b[1;33m                     level, max_level))\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Level value of 2 is too high.  Maximum allowed is 0."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from pywt import wavedec, waverec\n",
    "\n",
    "\"\"\"\n",
    "run the graph\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"./logs/SAE_test8\", graph=tf.get_default_graph())\n",
    "    writer.add_graph(sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #plt.ion()\n",
    "    #plt.figure()\n",
    "    #plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(numEpochs):\n",
    "        x,y = generateData()  \n",
    "        _current_cell_state = np.zeros((batchSize, stateSize))\n",
    "        _current_hidden_state = np.zeros((batchSize, stateSize))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * backpropagationLength\n",
    "            end_idx = start_idx + backpropagationLength\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]            \n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            coeffs = wavedec(batchX, 'haar', level=2)\n",
    "            reconstruct_X = waverec(coeffs, 'haar')\n",
    "            \n",
    "            #_, SAE_cost1 = sess.run([optmzr1, cost1], feed_dict={batchXHolder: reconstruct_X, batchYHolder: batchY})\n",
    "            #_, SAE_cost2 = sess.run([optmzr2, cost2], feed_dict={batchXHolder: reconstruct_X, batchYHolder: batchY})\n",
    "            #_, SAE_cost3 = sess.run([optmzr3, cost3], feed_dict={batchXHolder: reconstruct_X, batchYHolder: batchY})\n",
    "            #_, SAE_cost4 = sess.run([optmzr4, cost4], feed_dict={batchXHolder: reconstruct_X, batchYHolder: batchY}) \n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, currentState, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchXHolder:reconstruct_X,\n",
    "                    batchYHolder:batchY,\n",
    "                    cellState: _current_cell_state,\n",
    "                    hiddenState: _current_hidden_state\n",
    "                })\n",
    "\n",
    "            _current_cell_state, _current_hidden_state = _current_state\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            # fix the cost summary later\n",
    "            tf.summary.scalar(name=\"totalloss\", tensor=_total_loss)\n",
    "\n",
    "            print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "            #plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
